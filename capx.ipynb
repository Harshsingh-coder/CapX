{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(client_id='bIdJ1QDq79UqCLfNxrBYKQ',\n",
    "                     client_secret='lBdwiB37II3gb-Lyzj19VixpWXG-cg',\n",
    "                     user_agent='HarshSinghApp/0.1 by u/TransitionFresh1818')\n",
    "\n",
    "# Specify the subreddit and search term\n",
    "subreddit = reddit.subreddit('WallStreetBets')\n",
    "posts = []\n",
    "\n",
    "# Scrape top 1000 posts from subreddit\n",
    "for post in subreddit.top(limit=1000):\n",
    "    posts.append([post.title, post.selftext, post.score, post.num_comments, post.created])\n",
    "\n",
    "# Convert scraped data into a DataFrame\n",
    "posts_df = pd.DataFrame(posts, columns=['title', 'body', 'score', 'comments', 'created'])\n",
    "print(posts_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Handle missing data\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Return the cleaned text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cleaned_title cleaned_body\n",
      "0                times square right             \n",
      "1  upvote everyone sees got support             \n",
      "2               gme yolo update jan             \n",
      "3      gme yolo monthend update jan             \n",
      "4                           treason             \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to 'title' and 'body' columns\n",
    "posts_df['cleaned_title'] = posts_df['title'].apply(preprocess_text)\n",
    "posts_df['cleaned_body'] = posts_df['body'].apply(preprocess_text)\n",
    "\n",
    "# Handle missing data (e.g., empty titles or bodies) by removing rows with missing 'cleaned_body'\n",
    "posts_df.dropna(subset=['cleaned_body'], inplace=True)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "posts_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the cleaned DataFrame (first few rows)\n",
    "print(posts_df[['cleaned_title', 'cleaned_body']].head())\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file (optional)\n",
    "posts_df.to_csv('cleaned_reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "posts_df.to_csv('reddit_posts.csv', index=False)\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "posts_df.to_excel('reddit_posts.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title body   score  comments  \\\n",
      "0                  Times Square right now  NaN  487815     13997   \n",
      "1  UPVOTE so everyone sees we got SUPPORT  NaN  338183     12851   \n",
      "2           GME YOLO update — Jan 28 2021  NaN  300489     23039   \n",
      "3    GME YOLO month-end update — Jan 2021  NaN  264678     19918   \n",
      "4                       It’s treason then  NaN  247130      4601   \n",
      "\n",
      "        created                     cleaned_title cleaned_body  \n",
      "0  1.612030e+09                times square right          NaN  \n",
      "1  1.611841e+09  upvote everyone sees got support          NaN  \n",
      "2  1.611868e+09               gme yolo update jan          NaN  \n",
      "3  1.611954e+09      gme yolo monthend update jan          NaN  \n",
      "4  1.611964e+09                           treason          NaN  \n",
      "                                    title body   score  comments     created  \\\n",
      "0                  Times Square right now  NaN  487815     13997  1612029638   \n",
      "1  UPVOTE so everyone sees we got SUPPORT  NaN  338183     12851  1611841234   \n",
      "2           GME YOLO update — Jan 28 2021  NaN  300489     23039  1611867983   \n",
      "3    GME YOLO month-end update — Jan 2021  NaN  264678     19918  1611954285   \n",
      "4                       It’s treason then  NaN  247130      4601  1611963659   \n",
      "\n",
      "                      cleaned_title cleaned_body  \n",
      "0                times square right          NaN  \n",
      "1  upvote everyone sees got support          NaN  \n",
      "2               gme yolo update jan          NaN  \n",
      "3      gme yolo monthend update jan          NaN  \n",
      "4                           treason          NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = pd.read_csv('reddit_posts.csv')\n",
    "\n",
    "# Read the Excel file\n",
    "df_excel = pd.read_excel('reddit_posts.xlsx')\n",
    "\n",
    "# Display the data\n",
    "print(df_csv.head())\n",
    "print(df_excel.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='reddit_posts.csv' target='_blank'>reddit_posts.csv</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\Harsh Singh\\reddit_posts.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='reddit_posts.xlsx' target='_blank'>reddit_posts.xlsx</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\Harsh Singh\\reddit_posts.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a link to download the CSV file\n",
    "display(FileLink('reddit_posts.csv'))\n",
    "\n",
    "# Create a link to download the Excel file\n",
    "display(FileLink('reddit_posts.xlsx'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
