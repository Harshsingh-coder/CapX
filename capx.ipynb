{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title body   score  comments       created\n",
      "0                  Times Square right now       487822     13997  1.612030e+09\n",
      "1  UPVOTE so everyone sees we got SUPPORT       338172     12851  1.611841e+09\n",
      "2           GME YOLO update — Jan 28 2021       300493     23039  1.611868e+09\n",
      "3    GME YOLO month-end update — Jan 2021       264664     19918  1.611954e+09\n",
      "4                       It’s treason then       247138      4601  1.611964e+09\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Reddit API authentication\n",
    "reddit = praw.Reddit(client_id='bIdJ1QDq79UqCLfNxrBYKQ',\n",
    "                     client_secret='lBdwiB37II3gb-Lyzj19VixpWXG-cg',\n",
    "                     user_agent='HarshSinghApp/0.1 by u/TransitionFresh1818')\n",
    "\n",
    "# Specify the subreddit and search term\n",
    "subreddit = reddit.subreddit('WallStreetBets')\n",
    "posts = []\n",
    "\n",
    "# Scrape top 1000 posts from subreddit\n",
    "for post in subreddit.top(limit=1000):\n",
    "    posts.append([post.title, post.selftext, post.score, post.num_comments, post.created])\n",
    "\n",
    "# Convert scraped data into a DataFrame\n",
    "posts_df = pd.DataFrame(posts, columns=['title', 'body', 'score', 'comments', 'created'])\n",
    "print(posts_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Harsh\n",
      "[nltk_data]     Singh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Handle missing data\n",
    "    if not isinstance(text, str) or text.strip() == '':\n",
    "        return ''\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Return the cleaned text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cleaned_title cleaned_body\n",
      "0                times square right             \n",
      "1  upvote everyone sees got support             \n",
      "2               gme yolo update jan             \n",
      "3      gme yolo monthend update jan             \n",
      "4                           treason             \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to 'title' and 'body' columns\n",
    "posts_df['cleaned_title'] = posts_df['title'].apply(preprocess_text)\n",
    "posts_df['cleaned_body'] = posts_df['body'].apply(preprocess_text)\n",
    "\n",
    "# Handle missing data (e.g., empty titles or bodies) by removing rows with missing 'cleaned_body'\n",
    "posts_df.dropna(subset=['cleaned_body'], inplace=True)\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "posts_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the cleaned DataFrame (first few rows)\n",
    "print(posts_df[['cleaned_title', 'cleaned_body']].head())\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file (optional)\n",
    "posts_df.to_csv('cleaned_reddit_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV\n",
    "posts_df.to_csv('reddit_posts.csv', index=False)\n",
    "\n",
    "# Save the DataFrame to Excel\n",
    "posts_df.to_excel('reddit_posts.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    title body   score  comments  \\\n",
      "0                  Times Square right now  NaN  487827     13997   \n",
      "1  UPVOTE so everyone sees we got SUPPORT  NaN  338176     12851   \n",
      "2           GME YOLO update — Jan 28 2021  NaN  300493     23039   \n",
      "3    GME YOLO month-end update — Jan 2021  NaN  264677     19918   \n",
      "4                       It’s treason then  NaN  247126      4601   \n",
      "\n",
      "        created                     cleaned_title cleaned_body  \n",
      "0  1.612030e+09                times square right          NaN  \n",
      "1  1.611841e+09  upvote everyone sees got support          NaN  \n",
      "2  1.611868e+09               gme yolo update jan          NaN  \n",
      "3  1.611954e+09      gme yolo monthend update jan          NaN  \n",
      "4  1.611964e+09                           treason          NaN  \n",
      "                                    title body   score  comments     created  \\\n",
      "0                  Times Square right now  NaN  487827     13997  1612029638   \n",
      "1  UPVOTE so everyone sees we got SUPPORT  NaN  338176     12851  1611841234   \n",
      "2           GME YOLO update — Jan 28 2021  NaN  300493     23039  1611867983   \n",
      "3    GME YOLO month-end update — Jan 2021  NaN  264677     19918  1611954285   \n",
      "4                       It’s treason then  NaN  247126      4601  1611963659   \n",
      "\n",
      "                      cleaned_title cleaned_body  \n",
      "0                times square right          NaN  \n",
      "1  upvote everyone sees got support          NaN  \n",
      "2               gme yolo update jan          NaN  \n",
      "3      gme yolo monthend update jan          NaN  \n",
      "4                           treason          NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df_csv = pd.read_csv('reddit_posts.csv')\n",
    "\n",
    "# Read the Excel file\n",
    "df_excel = pd.read_excel('reddit_posts.xlsx')\n",
    "\n",
    "# Display the data\n",
    "print(df_csv.head())\n",
    "print(df_excel.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='reddit_posts.csv' target='_blank'>reddit_posts.csv</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\Harsh Singh\\CapX\\reddit_posts.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='reddit_posts.xlsx' target='_blank'>reddit_posts.xlsx</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\Harsh Singh\\CapX\\reddit_posts.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a link to download the CSV file\n",
    "display(FileLink('reddit_posts.csv'))\n",
    "\n",
    "# Create a link to download the Excel file\n",
    "display(FileLink('reddit_posts.xlsx'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
